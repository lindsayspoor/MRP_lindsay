{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lindsay Spoor, s1983822\n",
    "Master Research Project 2023-2024\n",
    "MARL for QEC\n",
    "\n",
    "\n",
    "# Toric Code Game\n",
    "\n",
    "This notebook provides an overview of training and evaluating an RL agent (PPO using stable-baselines3) on a toric code environment. \n",
    "\n",
    "First, the agent will be trained with the specified parameter settings. Afterwards, the trained agent can be evaluated and benchmarked against the performance of a MWPM decoding algorithm. \n",
    "\n",
    "\n",
    "The setup of the game in this notebook is as follows:\n",
    "\n",
    "1. The environment introduces N initial (bit-flip only) errors on qubits on the dxd board\n",
    "2. Agent sees the board state: the outcome of syndrome measurements on the board\n",
    "3. Agent decodes by doing bit-flips (as many as it wants) on qubits on the board until all syndrome points are resolved\n",
    "4. Board is checked on logical errors and agent is rewarded according to the state of the game\n",
    "5. Repeat this process durign training and evaluating\n",
    "\n",
    "The agent is evaluated by calculating the success rate $p_s$, that is, the number of successes divided the total number of played games on the board. These results are benchmarked against the MWPM success rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toric Code Environment\n",
    "\n",
    "The toric code environment is a dxd board with on each vertex of the grid a qubit, in total 2dxd qubits, and dxd plaquette and star operators. The eigenvalues of the plaquettes and stars indicate syndromes when having an eigenvalue of -1, and not having a syndrome when having an eigenvalue of 1. The environment used for this setup is from this paper:\n",
    "\n",
    "https://arxiv.org/abs/2101.08093\n",
    "\n",
    "## Reward system\n",
    "\n",
    "After each action the agent takes it gets a reward: -1 for each step not resulting in an end state (= no syndromes left on the board), +10 for reaching an end state and not having any logical errors present on the board, +5 for reaching an end state but having a logical error on the board. \n",
    "\n",
    "A logical error is defined as a non-trivial loop of error strings on the board. The toric code is a surface code with periodic boundaries. Therefore, when error strings are connecting one side of a boundary to the other, this results in a non-trivial loop which causes a logical error on the board. The goal of the agent is therefore not only removing all syndrome points on the board, but also developing a clever strategy on how to prevent the board from having a logical error. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from (and adjusted to this use-case) https://github.com/condensedAI/neat-qec/blob/main/toric_game_env.py\n",
    "\n",
    "from __future__ import division\n",
    "from typing import Any\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "#import gym\n",
    "from gymnasium.utils import seeding\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "### Environment\n",
    "class ToricGameEnv(gym.Env):\n",
    "    '''\n",
    "    ToricGameEnv environment. Effective single player game.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, settings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            opponent: Fixed\n",
    "            board_size: board_size of the board to use\n",
    "        \"\"\"\n",
    "\n",
    "        self.settings=settings\n",
    "\n",
    "        self.board_size = settings['board_size']\n",
    "        self.channels = [0]\n",
    "        self.memory = False\n",
    "        self.error_rate = settings['error_rate']\n",
    "        self.logical_error_reward=settings['l_reward']\n",
    "        self.continue_reward=settings['c_reward']\n",
    "        self.success_reward=settings['s_reward']\n",
    "        self.mask_actions=settings['mask_actions']\n",
    "        self.N = settings['N']\n",
    "\n",
    "        # Keep track of the moves\n",
    "        self.qubits_flips = [[],[]]\n",
    "        self.initial_qubits_flips = [[],[]]\n",
    "\n",
    "\n",
    "        # Empty State\n",
    "        self.state = Board(self.board_size)\n",
    "        self.done = None\n",
    "\n",
    "        self.observation_space = spaces.MultiBinary(self.board_size*self.board_size) #dxd plaquettes on which we can view syndromes\n",
    "        self.action_space = spaces.discrete.Discrete(len(self.state.qubit_pos)) #0-2dxd qubits on which a bit-flip error can be introduced\n",
    "\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed1 = seeding.np_random(seed)\n",
    "        # Derive a random seed.\n",
    "        seed2 = seeding.hash_seed(seed1 + 1) % 2**32\n",
    "        return [seed1, seed2]\n",
    "\n",
    "    def action_masks(self):\n",
    "\n",
    "        self.action_masks_list=np.zeros((len(self.state.qubit_pos)))\n",
    "        self.action_masks_list[:]=False\n",
    "        for i in self.state.syndrome_pos:\n",
    "            a,b = i[0],i[1]\n",
    "            mask_coords = [[(a-1)%(2*self.state.size),b%(2*self.state.size)],[a%(2*self.state.size),(b-1)%(2*self.state.size)],[a%(2*self.state.size),(b+1)%(2*self.state.size)],[(a+1)%(2*self.state.size),b%(2*self.state.size)]]\n",
    "            for j in mask_coords:\n",
    "                qubit_number = self.state.qubit_pos.index(j)\n",
    "\n",
    "                self.action_masks_list[qubit_number]=True\n",
    "\n",
    "\n",
    "\n",
    "        self.action_masks_list=list(self.action_masks_list)\n",
    "\n",
    "        return self.action_masks_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def generate_errors(self):\n",
    "        # Reset the board state\n",
    "\n",
    "        self.state.reset()\n",
    "\n",
    "        # Let the opponent do it's initial evil\n",
    "        self.qubits_flips = [[],[]]\n",
    "        self.initial_qubits_flips = [[],[]]\n",
    "\n",
    "        self._set_initial_errors()\n",
    "\n",
    "        self.done = self.state.is_terminal()\n",
    "\n",
    "        self.reward = 0\n",
    "        if self.done:\n",
    "            self.reward = self.success_reward\n",
    "            if self.state.has_logical_error(self.initial_qubits_flips):\n",
    "                self.reward = self.logical_error_reward\n",
    "\n",
    "\n",
    "        return self.state.encode(self.channels, self.memory)\n",
    "\n",
    "    def reset(self, *, seed: int | None = None, options: dict[str, Any] | None = None) -> tuple[Any, dict[str, Any]]:\n",
    "         super().reset(seed=seed, options=options)\n",
    "\n",
    "         initial_observation = self.generate_errors()\n",
    "\n",
    "         return initial_observation, {'state': self.state, 'message':\"reset\"}\n",
    "\n",
    "\n",
    "    def close(self):\n",
    "        self.state = None\n",
    "\n",
    "    def render(self, mode=\"human\", close=False):\n",
    "        fig, ax = plt.subplots()\n",
    "        a=1/(2*self.board_size)\n",
    "\n",
    "        for i, p in enumerate(self.state.plaquet_pos):\n",
    "            if self.state.op_values[0][i]==1:\n",
    "                fc='darkorange'\n",
    "                plaq = plt.Polygon([[a*p[0], a*(p[1]-1)], [a*(p[0]+1), a*(p[1])], [a*p[0], a*(p[1]+1)], [a*(p[0]-1), a*p[1]] ], fc=fc)\n",
    "                ax.add_patch(plaq)\n",
    "\n",
    "        for i, p in enumerate(self.state.star_pos):\n",
    "            if self.state.op_values[1][i]==1:\n",
    "                fc = 'green'\n",
    "                plaq = plt.Polygon([[a*p[0], a*(p[1]-1)], [a*(p[0]+1), a*(p[1])], [a*p[0], a*(p[1]+1)], [a*(p[0]-1), a*p[1]] ], fc=fc)\n",
    "                ax.add_patch(plaq)\n",
    "\n",
    "        # Draw lattice\n",
    "        for x in range(self.board_size):\n",
    "            for y in range(self.board_size):\n",
    "                pos=(2*a*x, 2*a*y)\n",
    "                width=a*2\n",
    "                lattice = plt.Rectangle( pos, width, width, fc='none', ec='black' )\n",
    "                ax.add_patch(lattice)\n",
    "\n",
    "        for i, p in enumerate(self.state.qubit_pos):\n",
    "            pos=(a*p[0], a*p[1])\n",
    "            fc='darkgrey'\n",
    "            if self.state.qubit_values[0][i] == 1 and self.state.qubit_values[1][i] == 0:\n",
    "                fc='darkblue'\n",
    "            elif self.state.qubit_values[0][i] == 0 and self.state.qubit_values[1][i] == 1:\n",
    "                fc='darkred'\n",
    "            elif self.state.qubit_values[0][i] == 1 and self.state.qubit_values[1][i] == 1:\n",
    "                fc='darkmagenta'\n",
    "            circle = plt.Circle( pos , radius=a*0.25, ec='k', fc=fc)\n",
    "            ax.add_patch(circle)\n",
    "            plt.annotate(str(i), pos, fontsize=8, ha=\"center\")\n",
    "\n",
    "        ax.set_xlim([-.1,1.1])\n",
    "        ax.set_ylim([-.1,1.1])\n",
    "        ax.set_aspect(1)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def step(self, location,  without_illegal_actions=True):\n",
    "\n",
    "        '''\n",
    "        Args:\n",
    "            location: coord of the qubit to flip\n",
    "            operator: Pauli matrix to apply\n",
    "        Return:\n",
    "            observation: board encoding,\n",
    "            reward: reward of the game,\n",
    "            done: boolean,\n",
    "            info: state dict\n",
    "        '''\n",
    "\n",
    "\n",
    "\n",
    "        if self.done:\n",
    "            self.reward = self.success_reward\n",
    "            return self.state.encode(self.channels, self.memory), self.success_reward, True, False,{'state': self.state, 'message':\"success\"}\n",
    "\n",
    " \n",
    "        pauli_opt=0\n",
    "        pauli_X_flip=True\n",
    "        pauli_Z_flip=False\n",
    "\n",
    "\n",
    "        # Check if we flipped twice the same qubit\n",
    "\n",
    "        if not without_illegal_actions:\n",
    "            if pauli_X_flip and location in self.qubits_flips[0]:\n",
    "                return self.state.encode(self.channels, self.memory), -1000, True, False,{'state': self.state, 'message': \"illegal_action\"}\n",
    "            if pauli_Z_flip and location in self.qubits_flips[1]:\n",
    "                return self.state.encode(self.channels, self.memory), -1000, True, False,{'state': self.state, 'message': \"illegal_action\"}\n",
    "        \n",
    "\n",
    "\n",
    "        if pauli_X_flip:\n",
    "            self.qubits_flips[0].append(location)\n",
    "\n",
    "        if pauli_Z_flip:\n",
    "            self.qubits_flips[1].append(location)\n",
    "\n",
    "\n",
    "        \n",
    "        self.state.act(self.state.qubit_pos[location], pauli_opt)\n",
    "\n",
    "        # agent gets reward if it does an action but it does not result in a terminal board state\n",
    "        if not self.state.is_terminal():\n",
    "            self.done = False\n",
    "\n",
    "            return self.state.encode(self.channels, self.memory), self.continue_reward, False, False,{'state': self.state, 'message':\"continue\"}\n",
    "    \n",
    "\n",
    "        # We're in a terminal state. Reward depends on whether it has won or lost (=logical error)\n",
    "        self.done = True\n",
    "        if self.state.has_logical_error(self.initial_qubits_flips):\n",
    "            return self.state.encode(self.channels, self.memory), self.logical_error_reward, True, False,{'state': self.state, 'message':\"logical_error\"}\n",
    "        else:\n",
    "            return self.state.encode(self.channels, self.memory), self.success_reward, True, False,{'state': self.state, 'message':\"success\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _set_initial_errors(self):\n",
    "        ''' Set random initial errors with an %error_rate rate\n",
    "            but report only the syndrome\n",
    "        '''\n",
    "        # Probabilitic mode\n",
    "        # Pick random sites according to error rate\n",
    "\n",
    "        for q in self.state.qubit_pos:    \n",
    "            if np.random.rand() < self.error_rate:\n",
    "                #print(f\" qubit has bit flip error on {self.state.qubit_pos.index(q)}\")\n",
    "\n",
    "                pauli_opt = 0\n",
    "\n",
    "\n",
    "                pauli_X_flip = (pauli_opt==0 or pauli_opt==2)\n",
    "                pauli_Z_flip = (pauli_opt==1 or pauli_opt==2)\n",
    "\n",
    "\n",
    "                if pauli_X_flip:\n",
    "                    self.initial_qubits_flips[0].append( q )\n",
    "                if pauli_Z_flip:\n",
    "                    self.initial_qubits_flips[1].append( q )\n",
    "\n",
    "                self.state.act(q, pauli_opt)\n",
    "\n",
    "\n",
    "        # Now unflip the qubits, they're a secret\n",
    "        self.state.qubit_values = np.zeros((2, 2*self.board_size*self.board_size))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ToricGameEnvFixedErrs(ToricGameEnv):\n",
    "    '''This environment is designed to initialise a board with N initial bit-flip errors instead of \n",
    "    introducing errors on the board according to an error rate.'''\n",
    "    \n",
    "    def __init__(self, settings):\n",
    "        super().__init__(settings)\n",
    "        \n",
    "\n",
    "    def _set_initial_errors(self):\n",
    "        ''' Set random initial errors for N initial flips\n",
    "            but report only the syndrome\n",
    "        '''\n",
    "        # Probabilistic mode\n",
    "        for q in np.random.choice(len(self.state.qubit_pos), self.N, replace=False): \n",
    "\n",
    "            q = self.state.qubit_pos[q]\n",
    "\n",
    "            pauli_opt = 0\n",
    "\n",
    "            pauli_X_flip = (pauli_opt==0 or pauli_opt==2)\n",
    "            pauli_Z_flip = (pauli_opt==1 or pauli_opt==2)\n",
    "\n",
    "\n",
    "            if pauli_X_flip:\n",
    "                self.initial_qubits_flips[0].append( q )\n",
    "            if pauli_Z_flip:\n",
    "                self.initial_qubits_flips[1].append( q )\n",
    "\n",
    "\n",
    "            self.state.act(q, pauli_opt)\n",
    "\n",
    "        # Now unflip the qubits, they're a secret\n",
    "        self.state.qubit_values = np.zeros((2, 2*self.board_size*self.board_size))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Board(object):\n",
    "    '''\n",
    "    Basic Implementation of a ToricGame Board, actions are int [0,2*board_size**2)\n",
    "    o : qubit\n",
    "    P : plaquette operator\n",
    "    x : star operator\n",
    "\n",
    "    x--o---x---o---x---o---\n",
    "    |      |       |\n",
    "    o  P   o   P   o   P\n",
    "    |      |       |\n",
    "    x--o---x---o---x---o---\n",
    "    |      |       |\n",
    "    o  P   o   P   o   P\n",
    "    |      |       |\n",
    "    x--o---x---o---x---o---\n",
    "    |      |       |\n",
    "    o  P   o   P   o   P\n",
    "    |      |       |\n",
    "\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def component_positions(size):\n",
    "        qubit_pos   = [[x,y] for x in range(2*size) for y in range((x+1)%2, 2*size, 2)]\n",
    "        plaquet_pos = [[x,y] for x in range(1,2*size,2) for y in range(1,2*size,2)]\n",
    "        star_pos    = [[x,y] for x in range(0,2*size,2) for y in range(0,2*size,2)]\n",
    "\n",
    "        return qubit_pos, plaquet_pos, star_pos\n",
    "\n",
    "    def __init__(self, board_size):\n",
    "        self.size = board_size\n",
    "\n",
    "        # Real-space locations\n",
    "        self.qubit_pos, self.plaquet_pos, self.star_pos  = self.component_positions(self.size)\n",
    "\n",
    "\n",
    "\n",
    "        # Define here the logical error for efficiency\n",
    "        self.z1pos = [[0,x] for x in range(1, 2*self.size, 2)]\n",
    "        self.z2pos = [[y,0] for y in range(1, 2*self.size, 2)]\n",
    "        self.x1pos = [[1,x] for x in range(0, 2*self.size, 2)]\n",
    "        self.x2pos = [[y,1] for y in range(0, 2*self.size, 2)]\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        #self.board_state = np.zeros( (2, 2*self.size, 2*self.size) )\n",
    "        \n",
    "        self.qubit_values = np.zeros((2, 2*self.size*self.size))\n",
    "        self.op_values = np.zeros((2, self.size*self.size))\n",
    "\n",
    "        self.syndrome_pos = [] # Location of syndromes\n",
    "\n",
    "    \n",
    "\n",
    "    def act(self, coord, operator):\n",
    "        '''\n",
    "            Args: input action in the form of position [x,y]\n",
    "            coord: real-space location of the qubit to flip\n",
    "        '''\n",
    "\n",
    "        qubit_index=self.qubit_pos.index(coord)\n",
    "\n",
    "\n",
    "        # Flip it!\n",
    "\n",
    "        self.qubit_values[0][qubit_index] = (self.qubit_values[0][qubit_index] + 1) % 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Update the syndrome measurements\n",
    "        # Only need to incrementally change\n",
    "        # Find plaquettes that the flipped qubit is a part of\n",
    "\n",
    "\n",
    "        plaqs=[]\n",
    "\n",
    "        if coord[0] % 2 == 0:\n",
    "            plaqs += [ [ (coord[0] + 1) % (2*self.size), coord[1] ], [ (coord[0] - 1) % (2*self.size), coord[1] ] ]\n",
    "        else:\n",
    "            plaqs += [ [ coord[0], (coord[1] + 1) % (2*self.size) ], [ coord[0], (coord[1] - 1) % (2*self.size) ] ]\n",
    "\n",
    "\n",
    "\n",
    "        # Update syndrome positions\n",
    "        for plaq in plaqs:\n",
    "            if plaq in self.syndrome_pos:\n",
    "                self.syndrome_pos.remove(plaq)\n",
    "            else:\n",
    "                self.syndrome_pos.append(plaq)\n",
    "\n",
    "            # The plaquette or vertex operators are only 0 or 1\n",
    "            if plaq in self.star_pos:\n",
    "                op_index = self.star_pos.index(plaq)\n",
    "                channel = 1\n",
    "            elif plaq in self.plaquet_pos:\n",
    "                op_index = self.plaquet_pos.index(plaq)\n",
    "                channel = 0\n",
    "\n",
    "        \n",
    "            self.op_values[channel][op_index] = (self.op_values[channel][op_index] + 1) % 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def is_terminal(self):\n",
    "        # Not needed I think\n",
    "        #if len(self.get_legal_action()) == 0:\n",
    "        #    return True\n",
    "\n",
    "        # Are all syndromes removed?\n",
    "        return len(self.syndrome_pos) == 0\n",
    "\n",
    "    def has_logical_error(self, initialmoves,debug=False):\n",
    "        #if there are an odd number of qubit flips on the boundaries, there is a logical error.\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Check for Z logical error\n",
    "        zerrors = [0,0]\n",
    "        for pos in self.z1pos:\n",
    "            if pos in initialmoves[0]:\n",
    "                zerrors[0] += 1\n",
    "            qubit_index = self.qubit_pos.index(pos)\n",
    "            zerrors[0] += self.qubit_values[0][ qubit_index ]\n",
    "\n",
    "        for pos in self.z2pos:\n",
    "            if pos in initialmoves[0]:\n",
    "                zerrors[1] += 1\n",
    "            qubit_index = self.qubit_pos.index(pos)\n",
    "            zerrors[1] += self.qubit_values[0][ qubit_index ]\n",
    "\n",
    "        # Check for X logical error\n",
    "        xerrors = [0,0]\n",
    "        for pos in self.x1pos:\n",
    "            if pos in initialmoves[1]:\n",
    "                xerrors[0] += 1\n",
    "            qubit_index = self.qubit_pos.index(pos)\n",
    "            xerrors[0] += self.qubit_values[1][ qubit_index ]\n",
    "\n",
    "        for pos in self.x2pos:\n",
    "            if pos in initialmoves[1]:\n",
    "                xerrors[1] += 1\n",
    "            qubit_index = self.qubit_pos.index(pos)\n",
    "            xerrors[1] += self.qubit_values[1][ qubit_index ]\n",
    "\n",
    "\n",
    "\n",
    "        if (zerrors[0]%2 == 1) or (zerrors[1]%2 == 1) or \\\n",
    "            (xerrors[0]%2 == 1) or (xerrors[1]%2 == 1):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        ''' representation of the board class\n",
    "            print out board_state\n",
    "        '''\n",
    "        return f\"Qubit Values: {self.qubit_values}, Operator values: {self.op_values}\"\n",
    "\n",
    "    def encode(self, channels, memory):\n",
    "        '''Return: np array\n",
    "            np.array(board_size, board_size): state observation of the board\n",
    "        '''\n",
    "        # In case of uncorrelated noise for instance, we don't need information\n",
    "        # about the star operators\n",
    "\n",
    "        img=np.array([])\n",
    "        for channel in channels:\n",
    "            img = np.concatenate((img, self.op_values[channel]))\n",
    "            if memory:\n",
    "                img = np.concatenate((img, self.qubit_values[channel]))\n",
    "\n",
    "        return img\n",
    "\n",
    "    def image_view(self, number=False, channel=0):\n",
    "        image = np.empty((2*self.size, 2*self.size), dtype=object)\n",
    "\n",
    "        for i, plaq in enumerate(self.plaquet_pos):\n",
    "            if self.op_values[0][i] == 1:\n",
    "                image[plaq[0], plaq[1]] = \"P\"+str(i) if number else \"P\"\n",
    "            elif self.op_values[0][i] == 0:\n",
    "                image[plaq[0], plaq[1]] = \"x\"+str(i) if number else \"x\"\n",
    "        for i,plaq in enumerate(self.star_pos):\n",
    "            if self.op_values[1][i] == 1:\n",
    "                image[plaq[0], plaq[1]] = \"S\"+str(i) if number else \"S\"\n",
    "            elif self.op_values[1][i] == 0:\n",
    "                image[plaq[0], plaq[1]] = \"+\"+str(i) if number else \"+\"\n",
    "\n",
    "        for i,pos in enumerate(self.qubit_pos):\n",
    "            image[pos[0], pos[1]] = str(int(self.qubit_values[channel,i]))+str(i) if number else str(int(self.qubit_values[channel, i]))\n",
    "\n",
    "        return np.array(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Agent\n",
    "\n",
    "Using stable-baselines3 the agent is a PPO agent. The agent uses an action mask to mask out all actions that are not 'allowed', that is, the agent is only allowed to select actions (qubits) adjacent to syndrome points, in order to make convergence during training happening faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "import os\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.policies import obs_as_tensor\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "os.getcwd()\n",
    "\n",
    "\n",
    "\n",
    "#logger\n",
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, f\"best_model\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                    print(\n",
    "                        f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\"\n",
    "                    )\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"Saving new best model to {self.save_path}.zip\")\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True\n",
    "\n",
    "class PPO_agent:\n",
    "    def __init__(self, initialisation_settings, log):#path):\n",
    "\n",
    "        self.initialisation_settings=initialisation_settings\n",
    "        # Create log dir\n",
    "        self.log=log\n",
    "        if self.log:\n",
    "            self.log_dir = \"log_dir\"\n",
    "            os.makedirs(self.log_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "        #INITIALISE MODEL FOR INITIALISATION\n",
    "        self.initialise_model()\n",
    "\n",
    "    def initialise_model(self):\n",
    "        #INITIALISE ENVIRONMENT INITIALISATION\n",
    "        print(\"initialising the environment and model...\")\n",
    "        if self.initialisation_settings['fixed']:\n",
    "            self.env = ToricGameEnvFixedErrs(self.initialisation_settings)\n",
    "        else:\n",
    "            self.env = ToricGameEnv(self.initialisation_settings)\n",
    "\n",
    "\n",
    "        # Logs will be saved in log_dir/monitor.csv\n",
    "        if self.log:\n",
    "            self.env = Monitor(self.env, self.log_dir)\n",
    "            # Create the callback: check every ... steps\n",
    "            self.callback = SaveOnBestTrainingRewardCallback(check_freq=10000, log_dir=self.log_dir)\n",
    "            \n",
    "        #INITIALISE MODEL FOR INITIALISATION\n",
    "        if self.initialisation_settings['mask_actions']:\n",
    "            ppo = MaskablePPO\n",
    "            policy = MaskableActorCriticPolicy\n",
    "        else:\n",
    "            ppo= PPO\n",
    "            policy = MlpPolicy\n",
    "        \n",
    "        self.model = ppo(policy, self.env, ent_coef=self.initialisation_settings['ent_coef'], clip_range = self.initialisation_settings['clip_range'],learning_rate=self.initialisation_settings['lr'], verbose=0, policy_kwargs={\"net_arch\":dict(pi=[64,64], vf=[64,64])})\n",
    "\n",
    "        print(\"initialisation done\")\n",
    "        print(self.model.policy)\n",
    "\n",
    "    def change_environment_settings(self, settings):\n",
    "        print(\"changing environment settings...\")\n",
    "        if settings['fixed']:\n",
    "            self.env = ToricGameEnvFixedErrs(settings)\n",
    "        else:\n",
    "            self.env = ToricGameEnv(settings)\n",
    "\n",
    "        # Logs will be saved in log_dir/monitor.csv\n",
    "        if self.log:\n",
    "            self.env = Monitor(self.env, self.log_dir)\n",
    "            # Create the callback: check every 1000 steps\n",
    "            self.callback = SaveOnBestTrainingRewardCallback(check_freq=10000, log_dir=self.log_dir)\n",
    "        \n",
    "        self.model.set_env(self.env)\n",
    "\n",
    "        print(\"changing settings done\")\n",
    "\n",
    "    def train_model(self, save_model_path):\n",
    "        print(\"training the model...\")\n",
    "        if self.log:\n",
    "            self.model.learn(total_timesteps=self.initialisation_settings['total_timesteps'], progress_bar=True, callback=self.callback)\n",
    "            self.plot_results(self.log_dir, save_model_path)\n",
    "        else:\n",
    "            self.model.learn(total_timesteps=self.initialisation_settings['total_timesteps'], progress_bar=True)\n",
    "    \n",
    "        self.model.save(f\"model_ppo_{save_model_path}\")\n",
    "        print(\"training done\")\n",
    "\n",
    "    def load_model(self, load_model_path):\n",
    "        print(\"loading the model...\")\n",
    "        if self.initialisation_settings['mask_actions']:\n",
    "            self.model=MaskablePPO.load(f\"model_ppo_{load_model_path}\")\n",
    "        else:\n",
    "            self.model=PPO.load(f\"model_ppo_{load_model_path}\")\n",
    "        print(\"loading done\")\n",
    "    \n",
    "    def moving_average(self,values, window): #for plotting the learning curve\n",
    "        \"\"\"\n",
    "        Smooth values by doing a moving average\n",
    "        :param values: (numpy array)\n",
    "        :param window: (int)\n",
    "        :return: (numpy array)\n",
    "        \"\"\"\n",
    "        weights = np.repeat(1.0, window) / window\n",
    "        print(values)\n",
    "        return np.convolve(values, weights, \"valid\")\n",
    "\n",
    "    def plot_results(self,log_folder, save_model_path, title=\"Learning Curve\"): #for plotting the learning curve\n",
    "        \"\"\"\n",
    "        plot the results\n",
    "\n",
    "        :param log_folder: (str) the save location of the results to plot\n",
    "        :param title: (str) the title of the task to plot\n",
    "        \"\"\"\n",
    "        x, y = ts2xy(load_results(log_folder), \"timesteps\")\n",
    "        y = self.moving_average(y, window=50)\n",
    "        # Truncate x\n",
    "        x = x[len(x) - len(y) :]\n",
    "\n",
    "        fig = plt.figure(title)\n",
    "        plt.plot(x, y)\n",
    "        plt.yscale(\"linear\")\n",
    "        plt.xlabel(\"Number of Timesteps\")\n",
    "        plt.ylabel(\"Rewards\")\n",
    "        plt.title(title + \" Smoothed\")\n",
    "        plt.savefig(f'learning_curve_{save_model_path}.pdf')\n",
    "\n",
    "\n",
    "    def render(self, obs0_k,evaluation_settings, actions_k, initial_flips_k): #for rendering the environment during evaluation (if needed)\n",
    "        #renders both the MWPM strategy and the PPO agent strategy, as well as the initial bit flips introduced by the environment\n",
    "\n",
    "        size=evaluation_settings['board_size']\n",
    "        qubit_pos   = [[x,y] for x in range(2*size) for y in range((x+1)%2, 2*size, 2)]\n",
    "        plaquet_pos = [[x,y] for x in range(1,2*size,2) for y in range(1,2*size,2)]\n",
    "\n",
    "\n",
    "        fig, (ax3,ax1,ax2) = plt.subplots(1,3, figsize=(15,5))\n",
    "        a=1/(2*size)\n",
    "\n",
    "        for i, p in enumerate(plaquet_pos):\n",
    "            if obs0_k.flatten()[i]==1:\n",
    "\n",
    "                fc='darkorange'\n",
    "                plaq = plt.Polygon([[a*p[0], a*(p[1]-1)], [a*(p[0]+1), a*(p[1])], [a*p[0], a*(p[1]+1)], [a*(p[0]-1), a*p[1]] ], fc=fc)\n",
    "                ax1.add_patch(plaq)\n",
    "\n",
    "        for i, p in enumerate(plaquet_pos):\n",
    "            if obs0_k.flatten()[i]==1:\n",
    "\n",
    "                fc='darkorange'\n",
    "                plaq = plt.Polygon([[a*p[0], a*(p[1]-1)], [a*(p[0]+1), a*(p[1])], [a*p[0], a*(p[1]+1)], [a*(p[0]-1), a*p[1]] ], fc=fc)\n",
    "                ax2.add_patch(plaq)\n",
    "\n",
    "        for i, p in enumerate(plaquet_pos):\n",
    "            if obs0_k.flatten()[i]==1:\n",
    "\n",
    "                fc='darkorange'\n",
    "                plaq = plt.Polygon([[a*p[0], a*(p[1]-1)], [a*(p[0]+1), a*(p[1])], [a*p[0], a*(p[1]+1)], [a*(p[0]-1), a*p[1]] ], fc=fc)\n",
    "                ax3.add_patch(plaq)\n",
    "\n",
    "        # Draw lattice\n",
    "        for x in range(size):\n",
    "            for y in range(size):\n",
    "                pos=(2*a*x, 2*a*y)\n",
    "                width=a*2\n",
    "                lattice = plt.Rectangle( pos, width, width, fc='none', ec='black' )\n",
    "                ax1.add_patch(lattice)\n",
    "\n",
    "        for x in range(size):\n",
    "            for y in range(size):\n",
    "                pos=(2*a*x, 2*a*y)\n",
    "                width=a*2\n",
    "                lattice = plt.Rectangle( pos, width, width, fc='none', ec='black' )\n",
    "                ax2.add_patch(lattice)\n",
    "\n",
    "        for x in range(size):\n",
    "            for y in range(size):\n",
    "                pos=(2*a*x, 2*a*y)\n",
    "                width=a*2\n",
    "                lattice = plt.Rectangle( pos, width, width, fc='none', ec='black' )\n",
    "                ax3.add_patch(lattice)\n",
    "\n",
    "        for i, p in enumerate(qubit_pos):\n",
    "            pos=(a*p[0], a*p[1])\n",
    "            fc1='darkgrey'\n",
    "            if i in list(actions_k[:,0]):\n",
    "                fc1 = 'darkblue'\n",
    "\n",
    "\n",
    "            circle1 = plt.Circle( pos , radius=a*0.25, ec='k', fc=fc1)\n",
    "            ax1.add_patch(circle1)\n",
    "            ax1.annotate(str(i), pos, fontsize=8, ha=\"center\")\n",
    "        \n",
    "        for i, p in enumerate(qubit_pos):\n",
    "            pos=(a*p[0], a*p[1])\n",
    "            fc2='darkgrey'\n",
    "            if i in list(actions_k[:,1]):\n",
    "                fc2 = 'red'\n",
    "\n",
    "\n",
    "            circle2 = plt.Circle( pos , radius=a*0.25, ec='k', fc=fc2)\n",
    "            ax2.add_patch(circle2)\n",
    "            ax2.annotate(str(i), pos, fontsize=8, ha=\"center\")\n",
    "\n",
    "        for i, p in enumerate(qubit_pos):\n",
    "            pos=(a*p[0], a*p[1])\n",
    "            fc3='darkgrey'\n",
    "            if p in list(initial_flips_k)[0]:\n",
    "                fc3 = 'magenta'\n",
    "\n",
    "\n",
    "            circle3 = plt.Circle( pos , radius=a*0.25, ec='k', fc=fc3)\n",
    "            ax3.add_patch(circle3)\n",
    "            ax3.annotate(str(i), pos, fontsize=8, ha=\"center\")\n",
    "\n",
    "        ax1.set_xlim([-.1,1.1])\n",
    "        ax1.set_ylim([-.1,1.1])\n",
    "        ax1.set_aspect(1)\n",
    "        ax1.set_xticks([])\n",
    "        ax1.set_yticks([])\n",
    "        ax1.set_title(\"actions agent\")\n",
    "        ax2.set_xlim([-.1,1.1])\n",
    "        ax2.set_ylim([-.1,1.1])\n",
    "        ax2.set_aspect(1)\n",
    "        ax2.set_xticks([])\n",
    "        ax2.set_yticks([])\n",
    "        ax2.set_title(\"actions MWPM\")\n",
    "        ax1.axis('off')\n",
    "        ax2.axis('off')\n",
    "        ax3.set_xlim([-.1,1.1])\n",
    "        ax3.set_ylim([-.1,1.1])\n",
    "        ax3.set_aspect(1)\n",
    "        ax3.set_xticks([])\n",
    "        ax3.set_yticks([])\n",
    "        ax3.set_title(\"initial qubit flips\")\n",
    "        ax3.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    ################ MWPM Decoding functions ##################################\n",
    "\n",
    "    def matching_to_path(self,matchings, grid_q): #for MWPM decoding\n",
    "\n",
    "        \"\"\"TESTED(for 1 matching):Add path of matchings to qubit grid\n",
    "            input:\n",
    "                matchings: array with tuples of two matched stabilizers as elements(stabilizer = tuple of coords)\n",
    "                grid_q: grid of qubits with errors before correction\n",
    "            output:\n",
    "                grid_q: grid of qubits with all errors(correction=adding errors)\n",
    "            \"\"\"\n",
    "        L = len(grid_q[0])\n",
    "        for stab1, stab2 in matchings:\n",
    "            error_path = [0, 0]\n",
    "            row_dif = abs(stab1[0] - stab2[0])\n",
    "            if row_dif > L - row_dif:\n",
    "                # path through edge\n",
    "                error_path[0] += 1\n",
    "            col_dif = abs(stab1[1] - stab2[1])\n",
    "            if col_dif > L - col_dif:\n",
    "                # path through edge\n",
    "                error_path[1] += 1\n",
    "            last_row = stab1[0]\n",
    "            if stab1[0] != stab2[0]:  # not the same row\n",
    "                up_stab = min(stab1, stab2)\n",
    "                down_stab = max(stab1, stab2)\n",
    "                q_col = up_stab[1]  # column of the upper stabilizer\n",
    "                last_row = down_stab[0]\n",
    "                if error_path[0]:  # through edge\n",
    "                    for s_row in range(down_stab[0] - L, up_stab[0]):\n",
    "                        q_row = (s_row + 1) * 2  # row under current stabilizer\n",
    "                        grid_q[q_row][q_col] += 1  # make error = flip bit\n",
    "                else:\n",
    "                    for s_row in range(up_stab[0], down_stab[0]):\n",
    "                        q_row = (s_row + 1) * 2  # row under current stabilizer\n",
    "                        grid_q[q_row][q_col] += 1\n",
    "\n",
    "            if stab1[1] != stab2[1]:  # not the same col\n",
    "                left_stab = min(stab1, stab2, key=lambda x: x[1])\n",
    "                right_stab = max(stab1, stab2, key=lambda x: x[1])\n",
    "                q_row = 2 * last_row + 1\n",
    "                if error_path[1]:  # through edge\n",
    "                    for s_col in range(right_stab[1] - L, left_stab[1]):\n",
    "                        q_col = s_col + 1  # col right of stabilizer\n",
    "                        grid_q[q_row][q_col] += 1  # make error = flip bit\n",
    "                else:\n",
    "                    for s_col in range(left_stab[1], right_stab[1]):\n",
    "                        q_col = s_col + 1  # col right of stabilizer\n",
    "                        grid_q[q_row][q_col] += 1\n",
    "        return grid_q\n",
    "\n",
    "    def check_correction(self,grid_q): #for MWPM decoding\n",
    "        \"\"\"(tested for random ones):Check if the correction is correct(no logical X gates)\n",
    "        input:\n",
    "            grid_q: grid of qubit with errors and corrections\n",
    "        output:\n",
    "            corrected: boolean whether correction is correct.\n",
    "        \"\"\"\n",
    "        # correct if even times logical X1,X2=> even number of times through certain edges\n",
    "        # upper row = X1\n",
    "        if sum(grid_q[0]) % 2 == 1:\n",
    "            return (False, 'X1')\n",
    "        # odd rows = X2\n",
    "        if sum([grid_q[x][0] for x in range(1, len(grid_q), 2)]) == 1:\n",
    "            return (False, 'X2')\n",
    "\n",
    "        # and if all stabilizers give outcome +1 => even number of qubit flips for each stabilizer\n",
    "        # is this needed? or assume given stabilizer outcome is corrected for sure?\n",
    "        for row_idx in range(int(len(grid_q) / 2)):\n",
    "            for col_idx in range(len(grid_q[0])):\n",
    "                all_errors = 0\n",
    "                all_errors += grid_q[2 * row_idx][col_idx]  # above stabilizer\n",
    "                all_errors += grid_q[2 * row_idx + 1][col_idx]  # left of stabilizer\n",
    "                if row_idx < int(len(grid_q) / 2) - 1:  # not the last row\n",
    "                    all_errors += grid_q[2 * (row_idx + 1)][col_idx]\n",
    "                else:  # last row\n",
    "                    all_errors += grid_q[0][col_idx]\n",
    "                if col_idx < len(grid_q[2 * row_idx + 1]) - 1:  # not the last column\n",
    "                    all_errors += grid_q[2 * row_idx + 1][col_idx + 1]\n",
    "                else:  # last column\n",
    "                    all_errors += grid_q[2 * row_idx + 1][0]\n",
    "                if all_errors % 2 == 1:\n",
    "                    return (False, 'stab', row_idx, col_idx)  # stabilizer gives error -1\n",
    "\n",
    "        return (True, 'end')\n",
    "        # other way of checking: for each row, look if no errors on qubits, => no loop around torus,so no gate applied.\n",
    "        # and similar for columns\n",
    "\n",
    "    def decode_MWPM_method(self,obs0_k, initial_flips, evaluation_settings):\n",
    "\n",
    "\n",
    "        stab_errors = np.argwhere((obs0_k==1))\n",
    "\n",
    "\n",
    "        path_lengths = []\n",
    "\n",
    "        for stab1_idx in range(stab_errors.shape[0]-1):\n",
    "            for stab2_idx in range(stab1_idx + 1, stab_errors.shape[0]):\n",
    "                min_row_dif = min(abs(stab_errors[stab1_idx][0]-stab_errors[stab2_idx][0]), evaluation_settings['board_size']-abs(stab_errors[stab1_idx][0]-stab_errors[stab2_idx][0]))\n",
    "                min_col_dif = min(abs(stab_errors[stab1_idx][1]-stab_errors[stab2_idx][1]), evaluation_settings['board_size']-abs(stab_errors[stab1_idx][1]-stab_errors[stab2_idx][1]))\n",
    "\n",
    "                path_lengths.append([tuple(stab_errors[stab1_idx]),tuple(stab_errors[stab2_idx]), min_row_dif+min_col_dif])\n",
    "\n",
    "        G = nx.Graph()\n",
    "\n",
    "        for edge in path_lengths:\n",
    "            G.add_edge(edge[0],edge[1], weight=-edge[2])\n",
    "\n",
    "        matching = nx.algorithms.max_weight_matching(G, maxcardinality=True)\n",
    "\n",
    "        grid_q = [[0 for col in range(evaluation_settings['board_size'])] for row in range(2 * evaluation_settings['board_size'])]\n",
    "        grid_q=np.array(grid_q)\n",
    "\n",
    "        qubit_pos = AgentPPO.env.state.qubit_pos\n",
    "        for i in initial_flips[0]:\n",
    "            flip_index = [j==i for j in qubit_pos]\n",
    "            flip_index = np.reshape(flip_index, newshape=(2*evaluation_settings['board_size'], evaluation_settings['board_size']))\n",
    "            flip_index = np.argwhere(flip_index)\n",
    "\n",
    "            grid_q[flip_index[0][0],flip_index[0][1]]+=1 % 2\n",
    "        grid_q = list(grid_q)\n",
    "        grid_q_initial=np.copy(grid_q)\n",
    "\n",
    "\n",
    "        matched_error_grid = self.matching_to_path(matching, grid_q)\n",
    "\n",
    "\n",
    "        MWPM_grid = np.array(matched_error_grid)-grid_q_initial\n",
    "        MWPM_actions = np.argwhere(MWPM_grid.flatten()==1)\n",
    "\n",
    "        check = self.check_correction(matched_error_grid)\n",
    "\n",
    "\n",
    "    \n",
    "        return check[0], MWPM_actions\n",
    "\n",
    "\n",
    "    ################### Evaluation functions ##################################\n",
    "\n",
    "    def evaluate_model(self, evaluation_settings, render, number_evaluations, max_moves, check_fails):\n",
    "\n",
    "        print(\"evaluating the model...\")\n",
    "\n",
    "        moves=0\n",
    "        logical_errors=0\n",
    "        success=0\n",
    "        success_MWPM=0\n",
    "        logical_errors_MWPM=0\n",
    "\n",
    "        observations=np.zeros((number_evaluations, evaluation_settings['board_size']*evaluation_settings['board_size']))\n",
    "        results=np.zeros((number_evaluations,2)) #1st column for agent, 2nd column for MWPM decoder\n",
    "        actions=np.zeros((number_evaluations,max_moves,2)) #1st column for agent, 2nd column for MWPM decoder (3rd dimension)\n",
    "        actions[:,:,:]=np.nan\n",
    "        \n",
    "\n",
    "        for k in tqdm(range(number_evaluations)):\n",
    "            #print(\"new evaluation\")\n",
    "            obs, info = self.env.reset()\n",
    "            initial_flips = AgentPPO.env.initial_qubits_flips\n",
    "\n",
    "            obs0=obs.copy()\n",
    "            observations[k,:]=obs\n",
    "            obs0_k=obs0.reshape((evaluation_settings['board_size'],evaluation_settings['board_size']))\n",
    "\n",
    "            MWPM_check, MWPM_actions = self.decode_MWPM_method(obs0_k, initial_flips, evaluation_settings)\n",
    "\n",
    "            actions[k,:MWPM_actions.shape[0],1] = MWPM_actions[:,0]\n",
    "\n",
    "            if MWPM_check==True:\n",
    "                success_MWPM+=1\n",
    "                results[k,1]=1 #1 for success\n",
    "            if MWPM_check==False:\n",
    "                logical_errors_MWPM+=1\n",
    "                results[k,1]=0 #0 for fail\n",
    "\n",
    "            for i in range(max_moves):\n",
    "                if evaluation_settings['mask_actions']:\n",
    "                    action_masks=get_action_masks(self.env)\n",
    "\n",
    "                    action, _state = self.model.predict(obs, action_masks=action_masks, deterministic=True)\n",
    "\n",
    "                else:\n",
    "                    action, _state = self.model.predict(obs)\n",
    "                obs, reward, done, truncated, info = self.env.step(action)#, without_illegal_actions=True)\n",
    "                actions[k,i,0]=action\n",
    "                moves+=1\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                if done:\n",
    "                    if info['message']=='logical_error':\n",
    "                        if check_fails:\n",
    "                            if results[k,0]==0 and results[k,1]==1:\n",
    "                                \n",
    "                                print(info['message'])\n",
    "                                self.render(obs0_k,evaluation_settings, actions[k,:,:], initial_flips)\n",
    "\n",
    "                        logical_errors+=1\n",
    "                        results[k,0]=0 #0 for fail\n",
    "                    if info['message'] == 'success':\n",
    "                        success+=1\n",
    "                        results[k,0]=1 #1 for success\n",
    "\n",
    "                    break\n",
    "\n",
    "         \n",
    "        print(f\"mean number of moves per evaluation is {moves/number_evaluations}\")\n",
    "        \n",
    "        if (success+logical_errors)==0:\n",
    "            success_rate = 0\n",
    "        else:\n",
    "            success_rate= success / (success+logical_errors)\n",
    "\n",
    "        if (success_MWPM+logical_errors_MWPM)==0:\n",
    "            success_rate_MWPM = 0\n",
    "        else:\n",
    "            success_rate_MWPM= success_MWPM / (success_MWPM+logical_errors_MWPM)\n",
    "\n",
    "        print(\"evaluation done\")\n",
    "\n",
    "        return success_rate, success_rate_MWPM, observations, results, actions\n",
    "\n",
    "\n",
    "    def evaluate_fixed_errors(self, evaluation_settings, N_evaluates, render, number_evaluations, max_moves, check_fails, save_files):\n",
    "        \n",
    "        success_rates=[]\n",
    "        success_rates_MWPM=[]\n",
    "        observations_all=[]\n",
    "\n",
    "        for N_evaluate in N_evaluates:\n",
    "            print(f\"{N_evaluate=}\")\n",
    "            evaluation_settings['fixed'] = evaluate_fixed\n",
    "            evaluation_settings['N']=N_evaluate\n",
    "            evaluation_settings['success_reward']=evaluation_settings['N']\n",
    "            self.change_environment_settings(evaluation_settings)\n",
    "            success_rate, success_rate_MWPM, observations, results, actions = self.evaluate_model(evaluation_settings, render, number_evaluations, max_moves, check_fails)\n",
    "            success_rates.append(success_rate)\n",
    "            success_rates_MWPM.append(success_rate_MWPM)\n",
    "            observations_all.append(observations)\n",
    "            print(f\"{success_rate=}\")\n",
    "            print(f\"{success_rate_MWPM=}\")\n",
    "\n",
    "\n",
    "\n",
    "        success_rates=np.array(success_rates)\n",
    "        success_rates_MWPM=np.array(success_rates_MWPM)\n",
    "        observations_all=np.array(observations_all)\n",
    "        print(f\"{observations_all.shape=}\")\n",
    "\n",
    "\n",
    "        evaluation_path =''\n",
    "        for key, value in evaluation_settings.items():\n",
    "            evaluation_path+=f\"{key}={value}\"\n",
    "\n",
    "        if save_files:\n",
    "            if fixed:\n",
    "                np.savetxt(f\"success_rates_ppo_{evaluation_path}_{loaded_model_settings['N']}.csv\", success_rates)\n",
    "                np.savetxt(f\"success_rates_mwpm_{evaluation_path}_{loaded_model_settings['N']}.csv\", success_rates_MWPM)\n",
    "                np.savetxt(f\"observations_ppo_{evaluation_path}_{loaded_model_settings['N']}.csv\", observations)\n",
    "                np.savetxt(f\"results_ppo_{evaluation_path}_{loaded_model_settings['N']}.csv\", results)\n",
    "                np.savetxt(f\"actions_agent_ppo_{evaluation_path}_{loaded_model_settings['N']}.csv\", actions[:,:,0])\n",
    "                np.savetxt(f\"actions_MWPM_ppo_{evaluation_path}_{loaded_model_settings['N']}.csv\", actions[:,:,1])\n",
    "            else:\n",
    "                np.savetxt(f\"success_rates_ppo_{evaluation_path}_{loaded_model_settings['error_rate']}.csv\", success_rates)\n",
    "                np.savetxt(f\"success_rates_mwpm_{evaluation_path}_{loaded_model_settings['error_rate']}.csv\", success_rates_MWPM)\n",
    "                np.savetxt(f\"observations_ppo_{evaluation_path}_{loaded_model_settings['error_rate']}.csv\", observations)\n",
    "                np.savetxt(f\"results_ppo_{evaluation_path}_{loaded_model_settings['error_rate']}.csv\", results)\n",
    "                np.savetxt(f\"actions_agent_ppo_{evaluation_path}_{loaded_model_settings['error_rate']}.csv\", actions[:,:,0])\n",
    "                np.savetxt(f\"actions_MWPM_ppo_{evaluation_path}_{loaded_model_settings['error_rate']}.csv\", actions[:,:,1])\n",
    "\n",
    "        return success_rates, success_rates_MWPM,observations, results, actions\n",
    "    \n",
    "\n",
    "    def evaluate_error_rates(self,evaluation_settings, error_rates, render, number_evaluations, max_moves, check_fails, save_files, fixed):\n",
    "        success_rates=[]\n",
    "        success_rates_MWPM=[]\n",
    "        observations_all=[]\n",
    "\n",
    "        for error_rate in error_rates:\n",
    "            #SET SETTINGS TO EVALUATE LOADED AGENT ON\n",
    "            print(f\"{error_rate=}\")\n",
    "            evaluation_settings['error_rate'] = error_rate\n",
    "            evaluation_settings['fixed'] = evaluate_fixed\n",
    "\n",
    "            self.change_environment_settings(evaluation_settings)\n",
    "            success_rate, success_rate_MWPM, observations, results, actions = self.evaluate_model(evaluation_settings, render, number_evaluations, max_moves, check_fails)\n",
    "            success_rates.append(success_rate)\n",
    "            success_rates_MWPM.append(success_rate_MWPM)\n",
    "            observations_all.append(observations)\n",
    "            print(f\"{success_rate=}\")\n",
    "            print(f\"{success_rate_MWPM=}\")\n",
    "\n",
    "\n",
    "\n",
    "        success_rates=np.array(success_rates)\n",
    "        success_rates_MWPM=np.array(success_rates_MWPM)\n",
    "        observations_all=np.array(observations_all)\n",
    "\n",
    "\n",
    "\n",
    "        evaluation_path =''\n",
    "        for key, value in evaluation_settings.items():\n",
    "            evaluation_path+=f\"{key}={value}\"\n",
    "\n",
    "        if save_files:\n",
    "            if fixed:\n",
    "                np.savetxt(f\"success_rates_ppo_{evaluation_path}_{loaded_model_settings['N']}.csv\", success_rates)\n",
    "                np.savetxt(f\"success_rates_mwpm_{evaluation_path}_{loaded_model_settings['N']}.csv\", success_rates_MWPM)\n",
    "                np.savetxt(f\"observations_ppo_{evaluation_path}_{loaded_model_settings['N']}.csv\", observations)\n",
    "                np.savetxt(f\"results_ppo_{evaluation_path}_{loaded_model_settings['N']}.csv\", results)\n",
    "                np.savetxt(f\"actions_agent_ppo_{evaluation_path}_{loaded_model_settings['N']}.csv\", actions[:,:,0])\n",
    "                np.savetxt(f\"actions_MWPM_ppo_{evaluation_path}_{loaded_model_settings['N']}.csv\", actions[:,:,1])\n",
    "            else:\n",
    "                np.savetxt(f\"success_rates_ppo_{evaluation_path}_{loaded_model_settings['error_rate']}.csv\", success_rates)\n",
    "                np.savetxt(f\"success_rates_mwpm_{evaluation_path}_{loaded_model_settings['error_rate']}.csv\", success_rates_MWPM)\n",
    "                np.savetxt(f\"observations_ppo_{evaluation_path}_{loaded_model_settings['error_rate']}.csv\", observations)\n",
    "                np.savetxt(f\"results_ppo_{evaluation_path}_{loaded_model_settings['error_rate']}.csv\", results)\n",
    "                np.savetxt(f\"actions_agent_ppo_{evaluation_path}_{loaded_model_settings['error_rate']}.csv\", actions[:,:,0])\n",
    "                np.savetxt(f\"actions_MWPM_ppo_{evaluation_path}_{loaded_model_settings['error_rate']}.csv\", actions[:,:,1])\n",
    "\n",
    "        return success_rates, success_rates_MWPM,observations, results, actions\n",
    "\n",
    "\n",
    "########### Plot function for plotting agent success rate against MWPM success rate #####################\n",
    "\n",
    "def plot_benchmark_MWPM(success_rates_all, success_rates_all_MWPM,error_rates_eval, board_size,path_plot,agent_value_N, agent_value_error_rate,evaluate_fixed):\n",
    "    plt.figure()\n",
    "    #for j in range(success_rates.shape[0]):\n",
    "    if evaluate_fixed:\n",
    "        plt.plot(N_evaluates, success_rates_all_MWPM[-1,:]*100, label=f'd={board_size} MWPM decoder', linestyle='-.', linewidth=0.5, color='black')\n",
    "        plt.scatter(N_evaluates, success_rates_all[-1,:]*100, label=f\"d={board_size} PPO agent, N={agent_value_N}\", marker=\"^\", s=30)\n",
    "        plt.plot(N_evaluates, success_rates_all[-1,:]*100, linestyle='-.', linewidth=0.5)\n",
    "        plt.xlabel(r'N')\n",
    "    else:\n",
    "        plt.plot(error_rates_eval, success_rates_all_MWPM[-1,:]*100, label=f'd={board_size} MWPM decoder', linestyle='-.', linewidth=0.5, color='black')\n",
    "        plt.scatter(error_rates_eval, success_rates_all[-1,:]*100, label=f\"d={board_size} PPO agent, p_error={agent_value_error_rate}\", marker=\"^\", s=30)\n",
    "        plt.plot(error_rates_eval, success_rates_all[-1,:]*100, linestyle='-.', linewidth=0.5)\n",
    "        plt.xlabel(r'p')\n",
    "    plt.title(r'Toric Code - PPO vs MWPM')\n",
    "    plt.ylabel(r'Correct[\\%] $p_s$')\n",
    "    plt.legend()\n",
    "    plt.savefig(path_plot)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETTINGS FOR RUNNING THIS SCRIPT\n",
    "\n",
    "'''\n",
    "In this cell the settings for training & evaluating the PPO agent can be specified;\n",
    "\n",
    "- train: True -> agent will be trained on the specified settings. False -> training disabled, when running the agent \n",
    "trained on the specified settings will be loaded for evaluation or further curriculum learning.\n",
    "- curriculum: True -> curriculum learning enabled. Agent will be loaded in on the specified settings and will be trained on the curriculum learning settings.\n",
    "- save_files: True -> files will be saved, False -> files will not be saved\n",
    "- render: True -> renders board during evaluation on each step the agent takes\n",
    "- number_evaluations: number of evaluations the agent will be evaluated on. Results will be averaged over this number\n",
    "- max_moves: maximum number of actions the agent can take before the game will be ended if no terminal state is reached\n",
    "- evaluate: True -> agent will be evaluated. False -> no evaluation\n",
    "- check_fails: True -> during evaluation the script will show the cases in which the agent fails to succeed, but MWPM did well\n",
    "- board_size: size of the dxd board\n",
    "- error_rate: error rate of each of the qubits on the board\n",
    "- ent_coef: entropy coefficient to be specified for training the agent (exploration parameter)\n",
    "- clip_range: clipping parameter to be specified for training the PPO agent\n",
    "- N: number of initial moves the environment introduces at the beginning of each game\n",
    "- logical_error_reward: reward the agent receives when in a terminal state, but the result is a logical error\n",
    "- success_reward: reward the agent receives when in a terminal state, and the result is no logical error (=success)\n",
    "- continue reward: the reward the agent gets for each action that does not result in the terminal board state. \n",
    "If negative it gets penalized for each move it does, therefore giving the agent an incentive to remove syndromes in as less moves as possible.\n",
    "- total_timesteps: amount of timesteps the agent will be trained on if train=True, or the amount of timesteps the agent was trained on when loading this model\n",
    "- mask_actions: whether action masking is used (default=True)\n",
    "- log: True -> learning curve will be logged\n",
    "- evaluate_fixed: True -> agent will be evaluated on fixed amount of initial errors on the board. False -> agent will be evaluated on a board with initial errors based on an error rate\n",
    "- fixed: True -> agent will be trained on fixed amount of initial errors on the board. False -> agent will be trained on an error rate introducing initial errors on the board\n",
    "- N_evaluates: list with the number of fixed initial flips N the agent is evaluated on if evaluate_fixed=True\n",
    "- error_rates_eval: list with the error rates the agent is evaluated on if evaluate_fixed=False\n",
    "- N_curriculums: if curriculum=True the agent will be trained successively on each N initial flips in this list\n",
    "- error_rates_curriculum: if curriculum=True the agent will be trained successively on each error rate in this list\n",
    "\n",
    "\n",
    "If you want to do curriculum learning, specify the following:\n",
    "\n",
    "train=True\n",
    "curriculum=False\n",
    "\n",
    "set N=... to the first value you want curriculum learning to be on. (must correspond to first value in the list N_curriculum)\n",
    "\n",
    "N_curriculum=[...,...,...]\n",
    "\n",
    "After having the agent trained on the first value in N_curriculum, train will be automatically set to False (in order to load in the trained agent)\n",
    "ans curriculum will be set to True.\n",
    "\n",
    "If you just want to evaluate the agent on a trained model, set train=False, curriculum=False and specify N_evaluates=[...,...,...].\n",
    "\n",
    "'''\n",
    "\n",
    "train=True\n",
    "curriculum=False \n",
    "save_files=False\n",
    "render=False\n",
    "number_evaluations=10000\n",
    "max_moves=200\n",
    "evaluate=True\n",
    "check_fails=False\n",
    "\n",
    "board_size=5\n",
    "error_rate=0.01\n",
    "ent_coef=0.05\n",
    "clip_range=0.1\n",
    "N=1 \n",
    "logical_error_reward=5 \n",
    "success_reward=10 \n",
    "continue_reward=-1\n",
    "total_timesteps=100000\n",
    "learning_rate= 0.001\n",
    "mask_actions=True\n",
    "log = False\n",
    "fixed=True \n",
    "evaluate_fixed=True\n",
    "N_evaluates = [1,2,3,4,5] \n",
    "error_rates_eval=list(np.linspace(0.01,0.1,4))\n",
    "N_curriculums=[1,2,3,4]\n",
    "\n",
    "\n",
    "error_rates_curriculum=list(np.linspace(0.01,0.1,4))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#SET SETTINGS TO INITIALISE AGENT ON\n",
    "initialisation_settings = {'board_size': board_size,\n",
    "            'error_rate': error_rate,\n",
    "            'l_reward': logical_error_reward,\n",
    "            's_reward': success_reward,\n",
    "            'c_reward':continue_reward,\n",
    "            'lr':learning_rate,\n",
    "            'total_timesteps': total_timesteps,\n",
    "            'mask_actions': mask_actions,\n",
    "            'fixed':fixed,\n",
    "            'N':N,\n",
    "            'ent_coef':ent_coef,\n",
    "            'clip_range':clip_range\n",
    "            }\n",
    "\n",
    "#SET SETTINGS TO LOAD TRAINED AGENT ON\n",
    "loaded_model_settings = {'board_size': board_size,\n",
    "            'error_rate': error_rate,\n",
    "            'l_reward': logical_error_reward,\n",
    "            's_reward': success_reward,\n",
    "            'c_reward':continue_reward,\n",
    "            'lr':learning_rate,\n",
    "            'total_timesteps': total_timesteps,\n",
    "            'mask_actions': mask_actions,\n",
    "            'fixed':fixed,\n",
    "            'N':N,\n",
    "            'ent_coef':ent_coef,\n",
    "            'clip_range':clip_range\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "evaluation_settings = {'board_size': board_size,\n",
    "            'error_rate': error_rate,\n",
    "            'l_reward': logical_error_reward,\n",
    "            's_reward': success_reward,\n",
    "            'c_reward':continue_reward,\n",
    "            'lr':learning_rate,\n",
    "            'total_timesteps': total_timesteps,\n",
    "            'mask_actions': mask_actions,\n",
    "            'fixed':fixed,\n",
    "            'N':N,\n",
    "            'ent_coef':ent_coef,\n",
    "            'clip_range':clip_range\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fixed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m success_rates_all\u001b[39m=\u001b[39m[]\n\u001b[1;32m      5\u001b[0m success_rates_all_MWPM\u001b[39m=\u001b[39m[]\n\u001b[0;32m----> 8\u001b[0m \u001b[39mif\u001b[39;00m fixed:\n\u001b[1;32m      9\u001b[0m     curriculums\u001b[39m=\u001b[39mN_curriculums\n\u001b[1;32m     10\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fixed' is not defined"
     ]
    }
   ],
   "source": [
    "#run the script\n",
    "\n",
    "\n",
    "success_rates_all=[]\n",
    "success_rates_all_MWPM=[]\n",
    "\n",
    "\n",
    "if fixed:\n",
    "    curriculums=N_curriculums\n",
    "else:\n",
    "    curriculums=error_rates_curriculum\n",
    "\n",
    "\n",
    "for curriculum_val in curriculums:\n",
    "\n",
    "    if (train==True) and (curriculum == False) and(curriculums.index(curriculum_val)>0):\n",
    "        train=False\n",
    "        curriculum=True\n",
    "\n",
    "\n",
    "    save_model_path =''\n",
    "    for key, value in initialisation_settings.items():\n",
    "        save_model_path+=f\"{key}={value}\"\n",
    "\n",
    "\n",
    "    load_model_path =''\n",
    "    for key, value in loaded_model_settings.items():\n",
    "        load_model_path+=f\"{key}={value}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #initialise PPO Agent\n",
    "    AgentPPO = PPO_agent(initialisation_settings, log)\n",
    "\n",
    "    if train:\n",
    "        AgentPPO.train_model(save_model_path=save_model_path)\n",
    "    else:\n",
    "        print(f\"{loaded_model_settings['N']=}\")\n",
    "        AgentPPO.load_model(load_model_path=load_model_path)\n",
    "        \n",
    "\n",
    "    if curriculum:\n",
    "        if fixed:\n",
    "        \n",
    "            print(f\"{curriculum_val=}\")\n",
    "            initialisation_settings['N']=curriculum_val\n",
    "        else:\n",
    "            print(f\"{curriculum_val=}\")\n",
    "            initialisation_settings['error_rate']=curriculum_val\n",
    "\n",
    "\n",
    "\n",
    "        save_model_path =''\n",
    "        for key, value in initialisation_settings.items():\n",
    "            save_model_path+=f\"{key}={value}\"\n",
    "\n",
    "        AgentPPO.change_environment_settings(initialisation_settings)\n",
    "\n",
    "        AgentPPO.train_model(save_model_path=save_model_path)\n",
    "        \n",
    "        if fixed:\n",
    "            loaded_model_settings['N']=curriculum_val\n",
    "        else:\n",
    "            loaded_model_settings['error_rate']=curriculum_val\n",
    "\n",
    "\n",
    "\n",
    "    if evaluate:\n",
    "\n",
    "        if evaluate_fixed:\n",
    "            success_rates, success_rates_MWPM,observations, results, actions = AgentPPO.evaluate_fixed_errors(evaluation_settings, N_evaluates, render, number_evaluations, max_moves, check_fails, save_files)\n",
    "        else:\n",
    "            success_rates, success_rates_MWPM,observations, results, actions = AgentPPO.evaluate_error_rates(evaluation_settings, error_rates_eval, render, number_evaluations, max_moves, check_fails, save_files, fixed)\n",
    "\n",
    "\n",
    "        success_rates_all.append(success_rates)\n",
    "        success_rates_all_MWPM.append(success_rates_MWPM)\n",
    "\n",
    "\n",
    "\n",
    "evaluation_path =''\n",
    "for key, value in evaluation_settings.items():\n",
    "    evaluation_path+=f\"{key}={value}\"\n",
    "\n",
    "\n",
    "\n",
    "success_rates_all=np.array(success_rates_all)\n",
    "success_rates_all_MWPM=np.array(success_rates_all_MWPM)\n",
    "\n",
    "\n",
    "\n",
    "if fixed:\n",
    "    path_plot = f\"PPO_vs_MWPM_{evaluation_path}_{loaded_model_settings['N']}.pdf\"\n",
    "else:\n",
    "    path_plot = f\"PPO_vs_MWPM_{evaluation_path}_{loaded_model_settings['error_rate']}.pdf\"\n",
    "\n",
    "\n",
    "plot_benchmark_MWPM(success_rates_all, success_rates_all_MWPM, error_rates_eval, board_size,path_plot,loaded_model_settings['N'], loaded_model_settings['error_rate'],evaluate_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
